{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../utils')\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from modelsTF import *\n",
    "from loss import *\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (52563, 9, 32, 32, 1) --------> Output shape: (52563, 1, 96, 96, 1)\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "CLEAN_DATA_DIR = '/home/mark/DataBank/PROBA-V-CHKPT/trimmedPatchesDir'\n",
    "band = 'NIR'\n",
    "X_train = np.load(os.path.join(CLEAN_DATA_DIR, f'TRAINpatchesLR_{band}.npy'), allow_pickle=True)\n",
    "y_train = np.load(os.path.join(CLEAN_DATA_DIR, f'TRAINpatchesHR_{band}.npy'), allow_pickle=True)\n",
    "\n",
    "X_train = X_train.transpose((0, 3, 4, 2, 1))\n",
    "y_train = y_train.transpose((0, 3, 4, 2, 1))\n",
    "print(f'Input shape: {X_train.shape} --------> Output shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger('__name__')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Safety checks\n",
    "    if not os.path.exists(opt.ckptDir):\n",
    "        os.makedirs(opt.ckptDir)\n",
    "    if not os.path.exists(opt.logs):\n",
    "        os.makedirs(opt.logs)\n",
    "\n",
    "    logger.info('Building model...')\n",
    "    model = WDSRConv3D(scale=3, numFilters=32, kernelSize=(3, 3, 3), numResBlocks=8,\n",
    "                       expRate=8, decayRate=0.8, numImgLR=9, patchSizeLR=32)\n",
    "\n",
    "    if opt.optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=5e-4)\n",
    "    elif opt.optimizer == 'nadam':\n",
    "        # http://cs229.stanford.edu/proj2015/054_report.pdf\n",
    "        optimizer = Nadam(learning_rate=5e-4)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=5e-4)\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(step=tf.Variable(0),\n",
    "                                     psnr=tf.Variable(1.0),\n",
    "                                     optimizer=optimizer,\n",
    "                                     model=model)\n",
    "\n",
    "    checkpointManager = tf.train.CheckpointManager(checkpoint=checkpoint,\n",
    "                                                   directory=opt.ckptDir,\n",
    "                                                   max_to_keep=5)\n",
    "\n",
    "    # Load Data\n",
    "    logger.info('Loading data...')\n",
    "    patchLR = np.load(opt.patchLR, allow_pickle=True)\n",
    "    patchHR = np.load(opt.patchHR, allow_pickle=True)\n",
    "\n",
    "    X_train, X_val, y_train, y_val, y_train_mask, y_val_mask = train_test_split(\n",
    "        patchLR, patchHR, patchHR.mask, test_size=opt.split, random_state=17)\n",
    "\n",
    "    y = [y_train, y_train_mask]\n",
    "    valData = [X_val, [y_val, y_val_mask]]\n",
    "\n",
    "    # Initialize metrics\n",
    "    trainLoss = Mean(name='trainLoss')\n",
    "    trainPSNR = Mean(name='trainPSNR')\n",
    "    testLoss = Mean(name='testLoss')\n",
    "    testPSNR = Mean(name='testPSNR')\n",
    "\n",
    "    fitTrainData(model, optimizer, trainLoss, trainPSNR, testLoss, testPSNR,\n",
    "                 X_train, y, opt.batchSize, opt.epochs, 512, valData, 100,\n",
    "                 checkpoint, checkpointManager,\n",
    "                 opt.logDir, opt.ckptDir, opt.saveBestOnly)\n",
    "\n",
    "\n",
    "def fitTrainData(model, optimizer, trainLoss, trainPSNR, testLoss, testPSNR,\n",
    "                 X, y, batchSize, epochs, bufferSize, valData, valSteps,\n",
    "                 checkpoint, checkpointManager,\n",
    "                 logDir, ckptDir, saveBestOnly):\n",
    "\n",
    "    trainSet = loadTrainDataAsTFDataSet(X, y, epochs, batchSize, bufferSize)\n",
    "    valSet = loadValDataAsTFDataSet(valData[0], valData[1], valSteps, batchSize, bufferSize)\n",
    "\n",
    "    # Logger\n",
    "    w = tf.summary.create_file_writer(logDir)\n",
    "\n",
    "    dataSetLength = (*X, *y)[0].shape[0]\n",
    "    totalSteps = tf.cast(dataSetLength/batchSize, tf.int64)\n",
    "    globalStep = checkpoint.step\n",
    "    step = globalStep % totalSteps\n",
    "    epoch = 0\n",
    "\n",
    "    with w.as_default():\n",
    "        for x_batch_train, y_batch_train, y_mask_batch_train in trainSet:\n",
    "            if (totalSteps - step) == 0:\n",
    "                epoch += 1\n",
    "                step = globalStep % totalSteps\n",
    "                logger.info('Start of epoch %d' % (epoch))\n",
    "                # Reset metrics\n",
    "                trainLoss.reset_states()\n",
    "                trainPSNR.reset_states()\n",
    "                testLoss.reset_states()\n",
    "                testPSNR.reset_states()\n",
    "\n",
    "            step += 1\n",
    "            globalStep += 1\n",
    "            trainStep(x_batch_train, y_batch_train, y_mask_batch_train)\n",
    "            checkpoint.step.assign_add(1)\n",
    "\n",
    "            t = f\"step {step}/{int(totalSteps)}, loss: {trainLoss.result():.3f}, psnr: {trainPSNR.result():.3f}\"\n",
    "            logger.info(t)\n",
    "\n",
    "            tf.summary.scalar('Train PSNR', trainPSNR.result(), step=globalStep)\n",
    "\n",
    "            tf.summary.scalar('Train loss', trainLoss.result(), step=globalStep)\n",
    "\n",
    "            if step != 0 and (step % 100) == 0:\n",
    "                # Reset states for test\n",
    "                testLoss.reset_states()\n",
    "                testPSNR.reset_states()\n",
    "                for x_batch_val, y_batch_val, y_mask_batch_val in valSet:\n",
    "                    testStep(x_batch_val, y_batch_val, y_mask_batch_val, checkpoint)\n",
    "                tf.summary.scalar(\n",
    "                    'Test loss', testLoss.result(), step=globalStep)\n",
    "                tf.summary.scalar(\n",
    "                    'Test PSNR', testPSNR.result(), step=globalStep)\n",
    "                t = f\"Validation results... val_loss: {testLoss.result():.3f}, val_psnr: {testPSNR.result():.3f}\"\n",
    "                logger.info(t)\n",
    "                w.flush()\n",
    "\n",
    "                if saveBestOnly and (testPSNR.result() <= checkpoint.psnr):\n",
    "                    continue\n",
    "\n",
    "                checkpoint.psnr = testPSNR.result()\n",
    "                checkpointManager.save()\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def trainStep(lr, mean_lr, hr, mask, checkpoint, loss, metric, trainLoss, trainPSNR):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        sr = checkpoint.model([lr, mean_lr], training=True)\n",
    "        loss = loss(hr, sr, mask)\n",
    "\n",
    "    gradients = tape.gradient(loss, checkpoint.model.trainable_variables)\n",
    "    checkpoint.optimizer.apply_gradients(zip(gradients, checkpoint.model.trainable_variables))\n",
    "\n",
    "    metric = metric(hr, mask, sr)\n",
    "    trainLoss(loss)\n",
    "    trainPSNR(metric)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def testStep(lr, mean_lr, hr, mask, checkpoint, loss, metric, testLoss, testPSNR):\n",
    "    sr = checkpoint.model([lr, mean_lr], training=False)\n",
    "    loss = loss(hr, mask, sr)\n",
    "    metric = metric(hr, mask, sr)\n",
    "\n",
    "    testLoss(loss)\n",
    "    testPSNR(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WDSRConv3D(scale=3, numFilters=32, kernelSize=(3, 3, 3), numResBlocks=8,\n",
    "                expRate=8, decayRate=0.8, numImgLR=9, patchSizeLR=32, isGrayScale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Shape_3:0' shape=(4,) dtype=int32>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(Reshape((32,32,9))(Input(shape=(32, 32, 9, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
